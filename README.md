# ERA 2 Session 6 : Backpropagation 

## Requirement

### Part-1

Many of us are familiar with the basic concept of a neural network gradually optimizing itself through multiple iterations, minimizing loss, and consequently improving accuracy in predicting outcomes. It seems simple, possibly just a few lines of code. Yet, have we considered the challenges we would face without these tools? This holds true not only for us but for anyone worldwide involved in data mining, data analysis, machine learning, and related fields. The assignment might serve as a way to appreciate and acknowledge the efforts of those brilliant minds who pioneered these advancements, prompting us to reflect on the significance of this knowledge. And so, let's delve into the assignment with this perspective in mind.

![TwoLayerNN](Part1/Images/TwoLayerNN.png)

- Two input value 
- One Hidden layer of size 2 
- Output layer
- Two Output values 
- Optimize the model over several epochs and observe the reduction in loss.
- Conduct the identical experiment using various learning rates, then analyze and visually compare how each network learns.

### Part-2
Train a Neural Network on MNIST dataset.

#### Constraints
- Ensure that the parameters stay under 20,000.
- Execute the model precisely for 19 epochs.
- Incorporate Batch Normalization, Fully Connected (FC) Layer, and Global Average Pooling (GAP).
- Specify the dropout value.

#### Target Output
- Achieve an accuracy of 99.4% or higher within fewer than 20 epochs.

